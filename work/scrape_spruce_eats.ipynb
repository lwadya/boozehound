{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Spruce Eats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped drink recipes from the website www.thespruceeats.com in a two-stage process. I first collected a list of drink names along with their individual page URLs and base spirits. I then looped through the page URLs and scraped recipes, descriptions, and image links for each drink. From there I cleaned the data before pickling it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Functions\n",
    "* **var_to_pickle**: Writes the given variable to a pickle file\n",
    "* **read_pickle**: Reads the given pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from code.lw_pickle import var_to_pickle, read_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get List of Recipe Links\n",
    "Here I scrape drink names, URLs, and base spirits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_pk = '../data/se_drink_links.pk'\n",
    "drink_links = read_pickle(links_pk)\n",
    "\n",
    "# Scrape data only if pickle of links does not exist\n",
    "if not drink_links:\n",
    "    drink_links = []\n",
    "    url = 'https://www.thespruceeats.com/a-to-z-cocktail-recipes-3962886'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Gets list of all list items containing a beverage\n",
    "    list_items = soup.findAll('li', {'class':'', 'id':''})\n",
    "    for item in list_items:\n",
    "        \n",
    "        # Gets recipe url and skips item if there isn't one\n",
    "        a_tag = item.a\n",
    "        if not a_tag:\n",
    "            continue\n",
    "        url = a_tag.get('href').strip()\n",
    "        \n",
    "        # Gets base spirits and skips item if there aren't any\n",
    "        base_spirits = item.text.replace(a_tag.text, '').lower().strip()\n",
    "        if not base_spirits:\n",
    "            continue\n",
    "            \n",
    "        # Assigns name, base spirit, and recipe link to dictionary\n",
    "        name = a_tag.text.lower().strip()\n",
    "        drink_links.append({\n",
    "            'name':name,\n",
    "            'base_spirits':base_spirits,\n",
    "            'url':url\n",
    "        })\n",
    "        \n",
    "    # Writes out pickle of links\n",
    "    var_to_pickle(drink_links, links_pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create DataFrame and Clean Data\n",
    "I save the scraped data into a DataFrame and then clean the names and base spirits. I also eliminate any duplicates and URLs that do not lead to an individual recipe page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function that makes all necessary changes to a list of base spirits\n",
    "def clean_base_spirits(spirit_list):\n",
    "    liqueurs = ['chocolate', 'banana', 'melon', 'coffee']\n",
    "    out_list = []\n",
    "    for original in spirit_list:\n",
    "        if not original:\n",
    "            continue\n",
    "        revised = (unidecode(original).strip()\n",
    "                                      .replace('liqueurs', 'liqueur')\n",
    "                                      .replace('add vodka', 'vodka')\n",
    "                                      .replace('knob creek', 'whiskey'))\n",
    "        if revised in liqueurs:\n",
    "            revised += ' liqueur'\n",
    "        out_list.append(revised)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(drink_links)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cleans up names and adds column for name words\n",
    "df['name'] = (df['name'].str.replace('\\(.*\\)', '')\n",
    "                        .str.replace('\\s*&\\s*', ' and ')\n",
    "                        .str.replace('old fashioned', 'old-fashioned')\n",
    "                        .apply(unidecode)\n",
    "                        .str.strip())\n",
    "df['name_words'] = (df['name'].str.replace('[^a-z0-9 \\-]', '')\n",
    "                              .str.split())\n",
    "\n",
    "# Drops an extra, branded old-fashioned entry\n",
    "drop_idx = df[(df['name'] == 'old-fashioned') &\n",
    "              (df['url'].str.contains('knob-creek'))].index\n",
    "df.drop(drop_idx, inplace=True)\n",
    "\n",
    "# Adds some measures to check how close name is to url\n",
    "url_count = []\n",
    "url_percent = []\n",
    "for idx, row in df.iterrows():\n",
    "    matches = [word in row['url'] for word in row['name_words']]\n",
    "    url_count.append(sum(matches))\n",
    "    url_percent.append(url_count[-1] / len(matches))\n",
    "df['url_name_percent'] = url_percent\n",
    "df['url_name_count'] = url_count\n",
    "\n",
    "# Splits up base spirits into lists\n",
    "df['base_spirits'] = (df['base_spirits'].str.replace('[\\(\\)]', '')\n",
    "                                        .str.split(',| and | or ')\n",
    "                                        .apply(clean_base_spirits))\n",
    "\n",
    "# Remove specific recipes due to various issues\n",
    "drop_names = [\n",
    "    'corpse reviver',\n",
    "    'tornado cocktail']\n",
    "drop_urls = [\n",
    "    'https://www.verywellfamily.com/best-diaper-bags-4161109',\n",
    "    'https://www.thespruceeats.com/popular-brands-of-gin-to-try-4027227',\n",
    "    'https://www.thespruceeats.com/wonderful-winter-cocktails-4123837',\n",
    "    'https://www.thespruceeats.com/popular-brands-of-premium-vodka-759245',\n",
    "    'https://www.thespruceeats.com/hot-toddy-collection-759883',\n",
    "    'https://www.thespruceeats.com/christmas-cocktail-recipe-collection-759882',\n",
    "    'https://www.thespruceeats.com/fantastic-sangria-recipes-759875',\n",
    "    'https://www.thespruceeats.com/irish-whiskey-ginger-ale-beer-drinks-761457',\n",
    "    'https://www.thespruceeats.com/spring-cocktail-recipes-759873']\n",
    "df.drop(df[df['name'].isin(drop_names)].index, inplace=True)\n",
    "df.drop(df[df['url'].isin(drop_urls)].index, inplace=True)\n",
    "\n",
    "# Removes other duplicates by keeping only the names that best match their urls\n",
    "dup_df = df[df.duplicated(subset='url', keep=False)]\n",
    "df = df[~df.duplicated(subset='url', keep=False)]\n",
    "dup_df = dup_df.sort_values(by=['url', 'url_name_percent', 'url_name_count'],\n",
    "                            ascending=False)\n",
    "keepers_df = dup_df.groupby('url', as_index=False).first()\n",
    "df = df.reset_index(drop=True).append(keepers_df, sort=False)\n",
    "\n",
    "# Cleans up DataFrame by removing columns that are no longer necessary\n",
    "df = df.sort_values(by='name').reset_index(drop=True)\n",
    "df.drop(['url_name_percent', 'url_name_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get Recipes and Descriptions\n",
    "Next I scraped all individual drink page data, including descriptions, image URLs, ingredients, prep time, and instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_pk = '../data/se_recipes.pk'\n",
    "recipes = read_pickle(recipes_pk)\n",
    "\n",
    "# Scrape data only if pickle of links does not exist\n",
    "if not recipes:\n",
    "    recipes = []\n",
    "    for ind in df.index:\n",
    "        response = requests.get(df.loc[ind]['url'])\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        recipe_dict = {}\n",
    "        \n",
    "        # Gets description\n",
    "        tag = soup.find('div', {'id':'article__header--project_1-0'})\n",
    "        description = []\n",
    "        if tag:\n",
    "            for x in tag.findAll('div', {'class':'comp mntl-sc-block mntl-sc-block-html'}):\n",
    "                description.append(x.text.strip())\n",
    "        if description:\n",
    "            recipe_dict['description'] = ' '.join(description)\n",
    "        else:\n",
    "            recipe_dict['description'] = None\n",
    "            \n",
    "        # Gets image\n",
    "        image = soup.find('img', {'class':'figure__image js-figure-image'})\n",
    "        if image:\n",
    "            recipe_dict['image'] = image.get('src')\n",
    "        else:\n",
    "            recipe_dict['image'] = None\n",
    "        \n",
    "        # Gets ingredients\n",
    "        tag = soup.find('section', {'id':'section--ingredients_1-0'})\n",
    "        ingredients = []\n",
    "        if tag:\n",
    "            for x in tag.findAll('li'):\n",
    "                ingredients.append(x.text.strip())\n",
    "        recipe_dict['ingredients'] = ingredients\n",
    "        \n",
    "        # Gets prep time\n",
    "        try:\n",
    "            recipe_dict['prep_time'] = (soup.find('span', {'id':'meta-text_1-0'})\n",
    "                                            .find('span', {'class':'meta-text__data'})\n",
    "                                            .text)\n",
    "        except:\n",
    "            recipe_dict['prep_time'] = None\n",
    "        \n",
    "        # Gets instructions\n",
    "        tag = soup.find('section', {'id':'section--instructions_1-0'})\n",
    "        instructions = []\n",
    "        if tag:\n",
    "            for x in tag.findAll('div', {'class':'comp mntl-sc-block mntl-sc-block-html'}):\n",
    "                instructions.append(x.text.strip())\n",
    "        if instructions:\n",
    "            recipe_dict['instructions'] = ' '.join(instructions)\n",
    "        else:\n",
    "            recipe_dict['instructions'] = None\n",
    "        \n",
    "        recipes.append(recipe_dict)\n",
    "        \n",
    "        # Pause every 20 sites\n",
    "        if ind % 20 == 0:\n",
    "            time.sleep(10)\n",
    "        \n",
    "    # Writes out pickle of recipes\n",
    "    var_to_pickle(recipes, recipes_pk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Merge Dataframe\n",
    "Here I merge the per-drink data into the original DataFrame and remove the few entries that are missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(pd.DataFrame(recipes), left_index=True, right_index=True)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Clean Recipe Data\n",
    "I only cleaned the recipe columns that I ended up using for my model and app: description and ingredients. Later iterations could incorporate instructions and prep time, but here I just drop those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function that cleans lists of ingredients\n",
    "def clean_ingredients(ingredient_list):\n",
    "    out_list = []\n",
    "    for item in ingredient_list:\n",
    "        revised = unidecode(item)\n",
    "        revised = re.sub('\\s+', ' ', revised)\n",
    "        out_list.append(revised)\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['instructions', 'prep_time'], axis=1, inplace=True)\n",
    "df['description'] = df['description'].apply(unidecode)\n",
    "df['ingredients'] = df['ingredients'].apply(clean_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pickle DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pk = '../data/se_df.pk'\n",
    "var_to_pickle(df, df_pk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
